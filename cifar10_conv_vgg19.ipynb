{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o864MAI5ncPH"
      },
      "source": [
        "# Instalar Julia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_4voqAvCnWlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75180fd-dcd8-4216-ee92-83844d9e21b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Julia 1.9.1 on the current Colab Runtime...\n",
            "2023-06-19 19:44:12 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.9/julia-1.9.1-linux-x86_64.tar.gz [146318973/146318973] -> \"/tmp/julia.tar.gz\" [1]\n",
            "Installing Julia package IJulia...\n",
            "Installing Julia package BenchmarkTools...\n",
            "Installing Julia package LinearAlgebra...\n",
            "Installing Julia package Statistics...\n",
            "Installing Julia package Flux...\n",
            "Installing Julia package MLDatasets...\n",
            "Installing Julia package BetaML...\n",
            "Installing Julia package Printf...\n",
            "Installing Julia package BSON...\n",
            "Installing Julia package CUDA...\n",
            "Installing IJulia kernel...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.9\n",
            "\n",
            "Successfully installed julia version 1.9.1!\n",
            "Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\n",
            "jump to the 'Checking the Installation' section.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.9.1\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia BenchmarkTools LinearAlgebra Statistics Flux MLDatasets BetaML Printf BSON\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  nvidia-smi -L &> /dev/null && export GPU=1 || export GPU=0\n",
        "  if [ $GPU -eq 1 ]; then\n",
        "    JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia\n",
        "\n",
        "  echo ''\n",
        "  echo \"Successfully installed `julia -v`!\"\n",
        "  echo \"Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\"\n",
        "  echo \"jump to the 'Checking the Installation' section.\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLfFMVaDnhE6"
      },
      "source": [
        "# Codigo de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using LinearAlgebra, Statistics, Flux, MLDatasets, CUDA\n",
        "using BetaML: ConfusionMatrix, fit!, info\n",
        "using Printf, BSON"
      ],
      "metadata": {
        "id": "cgQGpdmSQvn3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treino\n",
        "class_names = MLDatasets.CIFAR10().metadata[\"class_names\"]\n",
        "x_treino, y_treino = MLDatasets.CIFAR10(Float32, split=:train)[:] |> gpu\n",
        "#x_treino = permutedims(x_treino, (2, 1, 3, 4));\n",
        "#x_treino = convert(CuArray{Float32,4}, x_treino);\n",
        "#x_treino = reshape(x_treino, (32, 32, 3, 50000));\n",
        "média_x_treino = mean(x_treino);\n",
        "desvio_x_treino = std(x_treino);\n",
        "x_treino = (x_treino .- média_x_treino) ./ desvio_x_treino;\n",
        "y_treino = Flux.onehotbatch(y_treino, 0:9)\n",
        "dados_treino = Flux.Data.DataLoader((x_treino, y_treino), batchsize=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1DZwQtvQzbo",
        "outputId": "d1df7451-0981-48f9-c563-4ebb5a9aad4d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391-element DataLoader(::Tuple{CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, OneHotArrays.OneHotMatrix{UInt32, CuArray{UInt32, 1, CUDA.Mem.DeviceBuffer}}}, batchsize=128)\n",
              "  with first element:\n",
              "  (32×32×3×128 CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, 10×128 OneHotMatrix(::CuArray{UInt32, 1, CUDA.Mem.DeviceBuffer}) with eltype Bool,)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste\n",
        "x_teste, y_teste = MLDatasets.CIFAR10(Float32, split=:test)[:] |> gpu\n",
        "#x_teste = permutedims(x_teste, (2, 1, 3, 4));\n",
        "#x_teste = convert(CuArray{Float32,4}, x_teste);\n",
        "#x_teste = reshape(x_teste, (32, 32, 3, 10000));\n",
        "média_x_teste = mean(x_teste);\n",
        "desvio_x_teste = std(x_teste);\n",
        "x_teste = (x_teste .- média_x_teste) ./ desvio_x_teste;\n",
        "\n",
        "y_teste = Flux.onehotbatch(y_teste, 0:9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ws32QGnQ1SF",
        "outputId": "486f3310-1728-4837-8978-f9d5e3403ab6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10×10000 OneHotMatrix(::CuArray{UInt32, 1, CUDA.Mem.DeviceBuffer}) with eltype Bool:\n",
              " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  …  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
              " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  1  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  1  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
              " ⋅  1  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bekvZY0knj_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54635ba-7954-47da-9e99-1420cddca2a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Chain(\n",
              "  Conv((3, 3), 3 => 64, relu, pad=1),   \u001b[90m# 1_792 parameters\u001b[39m\n",
              "  BatchNorm(64),                        \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\n",
              "  Conv((3, 3), 64 => 64, relu, pad=1),  \u001b[90m# 36_928 parameters\u001b[39m\n",
              "  BatchNorm(64),                        \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\n",
              "  MaxPool((2, 2)),\n",
              "  Conv((3, 3), 64 => 128, relu, pad=1),  \u001b[90m# 73_856 parameters\u001b[39m\n",
              "  BatchNorm(128),                       \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\n",
              "  Conv((3, 3), 128 => 128, relu, pad=1),  \u001b[90m# 147_584 parameters\u001b[39m\n",
              "  BatchNorm(128),                       \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\n",
              "  MaxPool((2, 2)),\n",
              "  Conv((3, 3), 128 => 256, relu, pad=1),  \u001b[90m# 295_168 parameters\u001b[39m\n",
              "  BatchNorm(256),                       \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\n",
              "  Conv((3, 3), 256 => 256, relu, pad=1),  \u001b[90m# 590_080 parameters\u001b[39m\n",
              "  BatchNorm(256),                       \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\n",
              "  Conv((3, 3), 256 => 256, relu, pad=1),  \u001b[90m# 590_080 parameters\u001b[39m\n",
              "  BatchNorm(256),                       \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 512\u001b[39m\n",
              "  Conv((3, 3), 256 => 256, relu, pad=1),  \u001b[90m# 590_080 parameters\u001b[39m\n",
              "  MaxPool((2, 2)),\n",
              "  Conv((3, 3), 256 => 512, relu, pad=1),  \u001b[90m# 1_180_160 parameters\u001b[39m\n",
              "  BatchNorm(512),                       \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\n",
              "  Conv((3, 3), 512 => 512, relu, pad=1),  \u001b[90m# 2_359_808 parameters\u001b[39m\n",
              "  BatchNorm(512),                       \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\n",
              "  Conv((3, 3), 512 => 512, relu, pad=1),  \u001b[90m# 2_359_808 parameters\u001b[39m\n",
              "  BatchNorm(512),                       \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\n",
              "  Conv((3, 3), 512 => 512, relu, pad=1),  \u001b[90m# 2_359_808 parameters\u001b[39m\n",
              "  MaxPool((2, 2)),\n",
              "  Conv((3, 3), 512 => 512, relu, pad=1),  \u001b[90m# 2_359_808 parameters\u001b[39m\n",
              "  BatchNorm(512),                       \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\n",
              "  Conv((3, 3), 512 => 512, relu, pad=1),  \u001b[90m# 2_359_808 parameters\u001b[39m\n",
              "  BatchNorm(512),                       \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\n",
              "  Conv((3, 3), 512 => 512, relu, pad=1),  \u001b[90m# 2_359_808 parameters\u001b[39m\n",
              "  BatchNorm(512),                       \u001b[90m# 1_024 parameters\u001b[39m\u001b[90m, plus 1_024\u001b[39m\n",
              "  Conv((3, 3), 512 => 512, relu, pad=1),  \u001b[90m# 2_359_808 parameters\u001b[39m\n",
              "  MaxPool((2, 2)),\n",
              "  var\"#5#6\"(),\n",
              "  Dense(512 => 4096, relu),             \u001b[90m# 2_101_248 parameters\u001b[39m\n",
              "  Dropout(0.5),\n",
              "  Dense(4096 => 4096, relu),            \u001b[90m# 16_781_312 parameters\u001b[39m\n",
              "  Dropout(0.5),\n",
              "  Dense(4096 => 10),                    \u001b[90m# 40_970 parameters\u001b[39m\n",
              "  NNlib.softmax,\n",
              ") \u001b[90m        # Total: 64 trainable arrays, \u001b[39m38_956_362 parameters,\n",
              "\u001b[90m          # plus 26 non-trainable, 8_448 parameters, summarysize \u001b[39m15.398 KiB."
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "modelo = Chain(\n",
        "    Conv((3, 3), 3 => 64, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(64),\n",
        "    Conv((3, 3), 64 => 64, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(64),\n",
        "    MaxPool((2,2)),\n",
        "    Conv((3, 3), 64 => 128, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(128),\n",
        "    Conv((3, 3), 128 => 128, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(128),\n",
        "    MaxPool((2,2)),\n",
        "    Conv((3, 3), 128 => 256, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(256),\n",
        "    Conv((3, 3), 256 => 256, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(256),\n",
        "    Conv((3, 3), 256 => 256, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(256),\n",
        "    Conv((3, 3), 256 => 256, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    MaxPool((2,2)),\n",
        "    Conv((3, 3), 256 => 512, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(512),\n",
        "    Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(512),\n",
        "    Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(512),\n",
        "    Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    MaxPool((2,2)),\n",
        "    Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(512),\n",
        "    Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(512),\n",
        "    Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    BatchNorm(512),\n",
        "    Conv((3, 3), 512 => 512, relu, pad=(1, 1), stride=(1, 1)),\n",
        "    MaxPool((2,2)),\n",
        "    x -> reshape(x, :, size(x, 4)),\n",
        "    Dense(512, 4096, relu),\n",
        "    Dropout(0.5),\n",
        "    Dense(4096, 4096, relu),\n",
        "    Dropout(0.5),\n",
        "    Dense(4096, 10),\n",
        "    softmax) |> gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acuracia(ŷ, y) = (mean(Flux.onecold(ŷ) .== Flux.onecold(y)))\n",
        "perda(x, y) = Flux.crossentropy(modelo(x), y)\n",
        "\n",
        "opt = Flux.ADAM(0.001)\n",
        "ps = Flux.params(modelo)\n",
        "\n",
        "num_épocas = 25\n",
        "melhor_acu = 0\n",
        "última_melhoria = 0\n",
        "\n",
        "for época in 1:num_épocas\n",
        "   println(\"Época \", época)\n",
        "   Flux.train!(perda, ps, dados_treino, opt)\n",
        "\n",
        "   ŷteste = modelo(x_teste)\n",
        "   acu = acuracia(ŷteste, y_teste)\n",
        "\n",
        "   @info(@sprintf(\"[%d]: Acurácia nos testes: %.4f\", época, acu))\n",
        "\n",
        "   if acu >= melhor_acu\n",
        "      global melhor_acu = acu\n",
        "      global última_melhoria = época\n",
        "   end\n",
        "   # Se a acurácia for muito boa, termine o treino\n",
        "   if acu >= 0.999\n",
        "      @info(\" -> Término prematuro: alcançamos uma acurácia de 99.9%\")\n",
        "      break\n",
        "   end\n",
        "\n",
        "   # Se não houve melhoria em 5 épocas, reduza a taxa de aprendizagem:\n",
        "   if época - última_melhoria >= 5 && opt.eta > 1e-6\n",
        "      opt.eta /= 10.0\n",
        "      @warn(\" -> Sem melhoria por enquanto, reduzindo a taxa de aprendizagem para $(opt.eta)!\")\n",
        "\n",
        "      # Após reduzir a taxa de aprendizagem, dê a ela umas poucas épocas para melhorar\n",
        "      última_melhoria = época\n",
        "   end\n",
        "\n",
        "   if época - última_melhoria >= 10\n",
        "      @warn(\" -> Consideramos que houve convergência.\")\n",
        "      break\n",
        "   end\n",
        "end\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITA1aD7nQ5UH",
        "outputId": "a2925eaa-3819-41e0-e74b-861ce991c2a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1\n",
            "Época 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[1]: Acurácia nos testes: 0.3708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[2]: Acurácia nos testes: 0.5554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[3]: Acurácia nos testes: 0.6798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[4]: Acurácia nos testes: 0.7383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[5]: Acurácia nos testes: 0.7510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[6]: Acurácia nos testes: 0.7691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[7]: Acurácia nos testes: 0.7890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[8]: Acurácia nos testes: 0.8165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[9]: Acurácia nos testes: 0.8175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[10]: Acurácia nos testes: 0.8089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[11]: Acurácia nos testes: 0.8286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[12]: Acurácia nos testes: 0.8298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[13]: Acurácia nos testes: 0.8304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[14]: Acurácia nos testes: 0.8276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[15]: Acurácia nos testes: 0.8328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[16]: Acurácia nos testes: 0.7981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[17]: Acurácia nos testes: 0.8380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[18]: Acurácia nos testes: 0.8407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[19]: Acurácia nos testes: 0.8339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[20]: Acurácia nos testes: 0.8280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[21]: Acurácia nos testes: 0.8408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[22]: Acurácia nos testes: 0.8429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[23]: Acurácia nos testes: 0.8418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[24]: Acurácia nos testes: 0.8455\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[25]: Acurácia nos testes: 0.8343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvando modelo"
      ],
      "metadata": {
        "id": "qWLNjzWZeeQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "let model = cpu(modelo)\n",
        "   BSON.@save \"./trained_model_cifar10.bson\" model\n",
        "end"
      ],
      "metadata": {
        "id": "m6i-ZeSnedZg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código de teste"
      ],
      "metadata": {
        "id": "ISXe-7-KT6jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inserção bibliotecas"
      ],
      "metadata": {
        "id": "QSy2WzbHUtcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using LinearAlgebra, Statistics, Flux, MLDatasets, CUDA\n",
        "using BetaML: ConfusionMatrix, fit!, info\n",
        "using Printf, BSON"
      ],
      "metadata": {
        "id": "fCNqMheNUs77"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando o modelo"
      ],
      "metadata": {
        "id": "Wqe_A2XiUe4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = BSON.@load \"trained_model_cifar10_83.bson\" model\n",
        "modelo = modelo |> gpu"
      ],
      "metadata": {
        "id": "gelmD7HUUO6t"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação dos dados de teste"
      ],
      "metadata": {
        "id": "t0NJFZndVFa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_teste, y_teste = MLDatasets.CIFAR10(Float32, split=:test)[:] |> gpu\n",
        "\n",
        "média_x_teste = mean(x_teste);\n",
        "desvio_x_teste = std(x_teste);\n",
        "x_teste = (x_teste .- média_x_teste) ./ desvio_x_teste;\n",
        "\n",
        "y_teste = Flux.onehotbatch(y_teste, 0:9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1-r1D6qVKdx",
        "outputId": "205de1d6-a575-441e-83e5-0e08c913fc00"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10×10000 OneHotMatrix(::CuArray{UInt32, 1, CUDA.Mem.DeviceBuffer}) with eltype Bool:\n",
              " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  …  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
              " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  1  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  1  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
              " ⋅  1  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
              " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando resultados do modelo a partir do conjunto de teste"
      ],
      "metadata": {
        "id": "3fes0Wi0WJPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ŷteste = modelo(x_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "DDBDWpvBWOWQ",
        "outputId": "fa3cace6-4713-4ce0-939d-b49ea263faa7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "MethodError: objects of type Nothing are not callable",
            "",
            "Stacktrace:",
            " [1] top-level scope",
            "   @ In[37]:1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriz de confusão\n"
      ],
      "metadata": {
        "id": "o8XcDpHTWCW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix()\n",
        "fit!(cm, Flux.onecold(y_teste) .- 1, Flux.onecold(ŷteste) .- 1)\n",
        "print(cm)\n",
        "\n",
        "res = info(cm)\n",
        "\n",
        "heatmap(string.(res[\"categories\"]),\n",
        "   string.(res[\"categories\"]),\n",
        "   res[\"normalised_scores\"],\n",
        "   seriescolor=cgrad([:white, :blue]),\n",
        "   xlabel=\"Predito\",\n",
        "   ylabel=\"Real\",\n",
        "   title=\"Matriz de Confusão (scores normalizados)\")\n",
        "\n",
        "# Limita o mapa de cores, para vermos melhor onde os erros estão\n",
        "\n",
        "heatmap(string.(res[\"categories\"]),\n",
        "   string.(res[\"categories\"]),\n",
        "   res[\"normalised_scores\"],\n",
        "   seriescolor=cgrad([:white, :blue]),\n",
        "   clim=(0.0, 0.02),\n",
        "   xlabel=\"Predito\",\n",
        "   ylabel=\"Real\",\n",
        "   title=\"Matriz de Confusão (scores normalizados)\")"
      ],
      "metadata": {
        "id": "cwHPJj1JWEW3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "julia 1.9.1",
      "name": "julia"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}